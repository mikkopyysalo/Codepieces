
# basic stuff. installing packages and creating the data frame
install.packages("tm")

install.packages("wordcloud")

library(tm)

library(wordcloud)

wttweets<-searchTwitter("#wisdomteeth",n=200)

df<-do.call("rbind", lapply(wttweets, as.data.frame)) 

#check dimensions
dim(df) 

df$text<-iconv(enc2utf8(df$text),sub="byte")

wtcorpus<-Corpus(VectorSource(df$text))

wtcorpus<-tm_map(wtcorpus,tolower, lazy=TRUE)

wtcorpus<-tm_map(wtcorpus,removePunctuation, lazy=TRUE)

wtcorpus<-tm_map(wtcorpus,removeNumbers, lazy=TRUE)

wtcorpus<-tm_map(wtcorpus,removeWords,stopwords("english"), lazy=TRUE)

wtcorpus<-tm_map(wtcorpus, content_transformer(function(x) iconv(x, to="UTF-8-MAC", sub="byte"),lazy=TRUE))

# building the document term matrix
wtdtm<-TermDocumentMatrix(wtcorpus, control=list(minWordLength=1))   


# some association analyses
findFreqTerms(wtdtm,lowfreg=10)
findAssocs(wtdtm, "pain", 0.30) 

# wordcloud
pilvi<-as.matrix(wtdtm)
cumulus<-sort(rowSums(pilvi), decreasing=TRUE)
nimet<-names(cumulus)
d<-data.frame(word=nimet, freq=cumulus)
wordcloud(d$word, d$freq, min.freq=3)

