
# basic stuff. installing packages and creating the data frame
install.packages("tm")

install.packages("wordcloud")

library(tm)

library(wordcloud)

wttweets<-searchTwitter("#wisdomteeth",n=200,cainfo="cacert.pem")

df<-do.call("rbind", lapply(wttweets, as.data.frame)) 

#check dimensions
dim(df) 

df$text<-iconv(enc2utf8(df$text),sub="byte")

wtcorpus<-Corpus(VectorSource(df$text))

wtcorpus<-tm_map(wtcorpus,tolower, lazy=TRUE)

wtcorpus<-tm_map(wtcorpus,removePunctuation, lazy=TRUE)

wtcorpus<-tm_map(wtcorpus,removeNumbers, lazy=TRUE)

wtcorpus<-tm_map(wtcorpus,removeWords,stopwords("english"), lazy=TRUE)
wtcorpus<-tm_map(wtcorpus,PlainTextDocument)
wtcorpus<-tm_map(wtcorpus, function(x) iconv(x, to="UTF-8-MAC", sub="byte"),lazy=TRUE)


# building the document term matrix
wtdtm<-TermDocumentMatrix(wtcorpus, control=list(minWordLength=1))   #At this stage errors appear: "invalid input" and some error message about "In parallel::mclapply(x, FUN, ...) :
  all scheduled cores encountered errors in user code" then rest of the code doesnÂ´t work  


# some association analyses
findFreqTerms(wtdtm,lowfreg=10)
findAssocs(wtdtm, "pain", 0.30) 

# wordcloud
pilvi<-as.matrix(wtdtm)
cumulus<-sort(rowSums(pilvi), decreasing=TRUE)
nimet<-names(cumulus)
d<-data.frame(word=nimet, freq=cumulus)
wordcloud(d$word, d$freq, min.freq=3)

